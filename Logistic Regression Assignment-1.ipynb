{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d89d61d-386e-4baf-886f-04da9bca73a3",
   "metadata": {},
   "source": [
    "Q1.Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528abb66-40da-4de3-9849-ca8566eaf774",
   "metadata": {},
   "source": [
    "Linear Regression and Logistic Regression are both machine learning algorithms used for different types of tasks and are suitable for distinct types of data.\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "Type of Output: Linear regression is used for predicting continuous numerical values. It models the relationship between the independent variables and the dependent variable, which is a continuous outcome. The output is a real-valued number.\n",
    "\n",
    "Use Cases: Linear regression is typically used for regression problems, such as predicting house prices based on features like square footage and number of bedrooms, or predicting a person's salary based on their years of experience.\n",
    "\n",
    "Equation: The equation of a simple linear regression model is:\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "Type of Output: Logistic regression is used for predicting binary outcomes (0 or 1, yes or no, true or false). It models the probability of a binary target variable.\n",
    "\n",
    "Use Cases: Logistic regression is suitable for classification problems. For example, it can be used to predict whether an email is spam or not spam based on features like the sender's address, subject line, and message content. Another example is predicting whether a patient has a disease (e.g., diabetes) or not based on medical test results.\n",
    "\n",
    "Equation: The logistic regression model uses the logistic function (sigmoid function) to model the probability of the binary outcome:\n",
    "\n",
    "p(y=1) = 1 / (1 + e^(-z))\n",
    "\n",
    "Where 'p(y=1)' is the probability of the positive class, 'e' is the base of the natural logarithm, and 'z' is a linear combination of the independent variables.\n",
    "\n",
    "Scenario where Logistic Regression is more appropriate:\n",
    "Let's consider a scenario where you want to predict whether a customer will make a purchase (yes or no) based on various customer attributes, such as age, income, browsing history, and previous purchase behavior. In this case, logistic regression would be more appropriate because it deals with binary classification problems.\n",
    "\n",
    "For instance, you can use logistic regression to build a model that predicts whether a website visitor will convert into a paying customer (1 for conversion, 0 for no conversion). The model will provide you with the probability of conversion for each visitor, allowing you to make decisions like optimizing the website for higher conversion rates or targeting specific marketing efforts towards visitors with a high probability of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c5eb6-373f-46b8-9161-ae4117b92d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d999d66-5f73-4b0a-86df-4db0ce186ada",
   "metadata": {},
   "source": [
    "In logistic regression, the cost function used is the Logistic Loss or Binary Cross-Entropy Loss function. This cost function is used to measure the error between the predicted probabilities and the actual binary outcomes (0 or 1) in a classification problem. The goal is to minimize this cost function to train the logistic regression model effectively.\n",
    "\n",
    "The Logistic Loss function for a single training example is defined as follows:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "L(y, ŷ) = - [ y * log(ŷ) + (1 - y) * log(1 - ŷ) ]\n",
    "\n",
    "Where:\n",
    "\n",
    "L(y, ŷ) is the logistic loss for a single example.\n",
    "y is the actual binary target (0 or 1).\n",
    "ŷ is the predicted probability that the target is 1.\n",
    "The cost function for the entire training dataset is the average of these individual losses:\n",
    "\n",
    "\n",
    "J(θ) = (1/m) * Σ[ -y * log(ŷ) - (1 - y) * log(1 - ŷ) ]\n",
    "\n",
    "Where:\n",
    "\n",
    "J(θ) is the cost function for the logistic regression model.\n",
    "m is the number of training examples.\n",
    "θ represents the model parameters (coefficients) that are being optimized.\n",
    "The objective during the training process is to find the model parameters θ that minimize the cost function J(θ).\n",
    "\n",
    "Optimizing the Cost Function (Minimizing J(θ)):\n",
    "\n",
    "The optimization process in logistic regression is typically achieved through an iterative optimization algorithm, most commonly the Gradient Descent algorithm. Here's how it works:\n",
    "\n",
    "1. Initialization: Start with an initial guess for the model parameters θ.\n",
    "\n",
    "2, Calculate Gradient: Compute the gradient of the cost function J(θ) with respect to each parameter θj. The gradient represents the direction and magnitude of the steepest ascent in the cost function space.\n",
    "\n",
    "3. Update Parameters: Update the parameters θ using the gradient. The general update rule for gradient descent is:\n",
    "\n",
    "\n",
    "θj = θj - α * (∂J(θ) / ∂θj)\n",
    "\n",
    "Where:\n",
    "\n",
    "α is the learning rate, a hyperparameter that controls the step size in each iteration.\n",
    "∂J(θ) / ∂θj is the partial derivative of the cost function with respect to parameter θj.\n",
    "\n",
    "4. Repeat: Continue iterating steps 2 and 3 until convergence is reached, which occurs when the cost function reaches a minimum or a small change in θ no longer significantly reduces the cost.\n",
    "\n",
    "5. Obtain the Trained Model: After convergence, you obtain the trained logistic regression model with optimized parameters θ, which can be used to make predictions on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105baf9-a86a-4c12-876e-a301d227d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225e095-3915-4304-978d-99d80afe2146",
   "metadata": {},
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting, which occurs when a model becomes too complex and fits the training data too closely, leading to poor generalization to new, unseen data. Regularization adds a penalty term to the logistic regression cost function, discouraging the model from assigning excessively large coefficients (weights) to features. This encourages the model to be simpler and reduces the risk of overfitting.\n",
    "\n",
    "There are two common types of regularization used in logistic regression:\n",
    "\n",
    " L1 Regularization (Lasso):\n",
    "\n",
    "In L1 regularization, a penalty term proportional to the absolute values of the coefficients is added to the cost function.\n",
    "The cost function with L1 regularization is often referred to as the \"Lasso\" cost function.\n",
    "L1 regularization encourages the model to produce sparse solutions, meaning it tends to set many feature coefficients to exactly zero. This can be useful for feature selection, as it effectively eliminates irrelevant features from the model.\n",
    "L1 regularization helps with both feature selection and regularization by preventing the model from relying too heavily on a subset of features.\n",
    " \n",
    " \n",
    " L2 Regularization (Ridge):\n",
    "\n",
    "In L2 regularization, a penalty term proportional to the square of the coefficients is added to the cost function.\n",
    "The cost function with L2 regularization is often referred to as the \"Ridge\" cost function.\n",
    "L2 regularization encourages the model to have small, non-zero coefficients for all features, rather than pushing any of them to exactly zero. This makes it a smoother regularization method compared to L1.\n",
    "L2 regularization helps prevent overfitting by shrinking the magnitude of the feature coefficients, making them less sensitive to variations in the training data.\n",
    "The overall cost function for logistic regression with regularization (L1 or L2) is a combination of the original logistic loss and the regularization term:\n",
    "\n",
    "J(θ) = [Original Logistic Loss] + [Regularization Term]\n",
    "The regularization term is determined by a hyperparameter called λ (lambda). The value of λ controls the strength of regularization. A larger λ will result in stronger regularization.\n",
    "\n",
    "How Regularization Prevents Overfitting:\n",
    "\n",
    "Regularization helps prevent overfitting in logistic regression by imposing a cost on the model for having large coefficients. Here's how it works:\n",
    "\n",
    "L1 and L2 regularization both add a penalty term to the cost function. This penalty term discourages the model from assigning excessively large weights to any feature.\n",
    "\n",
    "As a result, the optimization process during training seeks to find a balance between minimizing the logistic loss (fitting the data) and minimizing the regularization term (keeping the model simple).\n",
    "\n",
    "If the model starts to assign large coefficients to features that are not highly informative, the regularization term encourages the optimization algorithm to reduce those coefficients, effectively shrinking them.\n",
    "\n",
    "By shrinking the coefficients, the model becomes less sensitive to noise and fluctuations in the training data, leading to a smoother decision boundary and better generalization to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba48af7-691f-4147-af4e-c5e5a748758c",
   "metadata": {},
   "source": [
    "Q4.  What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c47b1-5795-4218-8a9c-44c5aca0f212",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical tool used to evaluate the performance of classification models, including logistic regression models. It provides a visual representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at different classification thresholds.\n",
    "\n",
    "Here's how the ROC curve is constructed and how it is used to evaluate the performance of a logistic regression model:\n",
    "\n",
    "Construction of the ROC Curve:\n",
    "\n",
    "1. Binary Classification Model: The ROC curve is typically used for binary classification problems, where there are two classes: positive (1) and negative (0).\n",
    "\n",
    "2. Model Prediction: The logistic regression model assigns a probability score (predicted probability) to each data point that it belongs to the positive class (e.g., class 1). These probability scores can be interpreted as confidence levels in the prediction.\n",
    "\n",
    "3. Threshold Variation: To construct the ROC curve, you vary the classification threshold (decision boundary) of the model. You start with a very low threshold (classifying nearly everything as positive) and gradually increase it until you reach a high threshold (classifying nearly everything as negative).\n",
    "\n",
    "4. Calculation: At each threshold, you calculate two important metrics:\n",
    "\n",
    "True Positive Rate (TPR) or Sensitivity: It is the proportion of true positive predictions (correctly predicted positive cases) among all actual positive cases.\n",
    "\n",
    "TPR = TP / (TP + FN)\n",
    "\n",
    "False Positive Rate (FPR) or 1 - Specificity: It is the proportion of false positive predictions (incorrectly predicted positive cases) among all actual negative cases.\n",
    "\n",
    "FPR = FP / (TN + FP)\n",
    "\n",
    "5. Plotting: For each threshold, you plot a point on the ROC curve with FPR on the x-axis and TPR on the y-axis. As you vary the threshold, you get a series of points that make up the ROC curve.\n",
    "\n",
    "Interpretation and Evaluation:\n",
    "\n",
    "A perfect classifier would have an ROC curve that passes through the upper left corner of the plot (0 FPR and 1 TPR), indicating high sensitivity and specificity.\n",
    "\n",
    "The closer the ROC curve is to the upper left corner, the better the model's performance. Conversely, a curve that hugs the diagonal line (45-degree line) suggests a model that is no better than random guessing.\n",
    "\n",
    "The area under the ROC curve (AUC-ROC) is a commonly used summary measure of a classifier's performance. It quantifies the overall ability of the model to discriminate between the two classes. A model with an AUC-ROC of 0.5 is no better than random guessing, while a model with an AUC-ROC of 1.0 is perfect.\n",
    "\n",
    "You can compare multiple models by comparing their ROC curves or AUC-ROC values. The model with the highest AUC-ROC is generally considered the best at discriminating between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156ff53-5dcf-4c2e-950d-90af0204bd67",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these \n",
    "techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67850276-2445-4df8-b700-a32744ba2869",
   "metadata": {},
   "source": [
    "\n",
    "Feature selection is a crucial step in building a logistic regression model. It involves choosing a subset of relevant features (input variables) while discarding irrelevant or redundant ones. Effective feature selection can improve the model's performance by reducing overfitting, decreasing training time, and increasing interpretability. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "1. Manual Feature Selection:\n",
    "\n",
    "This approach involves domain knowledge and expert intuition to select features based on their relevance to the problem.\n",
    "Features that are deemed irrelevant, redundant, or not contributing significantly to the model's performance are removed.\n",
    "\n",
    "2. Univariate Feature Selection:\n",
    "\n",
    "Univariate feature selection methods evaluate each feature's relationship with the target variable independently, without considering the interactions between features.\n",
    "Common metrics for this include chi-squared (for categorical variables), F-statistic (for numerical variables), and mutual information.\n",
    "\n",
    "3. Recursive Feature Elimination (RFE):\n",
    "\n",
    "RFE is an iterative method that starts with all features and repeatedly removes the least significant feature until a predefined number of features is reached. It relies on the model's performance (e.g., logistic regression) and is computationally more intensive.\n",
    "\n",
    "4. Feature Importance from Tree-Based Models:\n",
    "\n",
    "Tree-based models like Decision Trees and Random Forests can provide feature importance scores.\n",
    "Features with higher importance scores are more relevant and can be selected.\n",
    "\n",
    "5. L1 Regularization (Lasso):\n",
    "\n",
    "L1 regularization in logistic regression encourages sparsity by shrinking some feature coefficients to exactly zero.\n",
    "Features with non-zero coefficients after regularization are selected.\n",
    "\n",
    "6. Correlation-Based Feature Selection:\n",
    "\n",
    "Features that are highly correlated with the target variable are often considered more relevant.\n",
    "However, highly correlated features with each other may be redundant, so one of them is removed.\n",
    "\n",
    "7. Wrapper Methods:\n",
    "\n",
    "Wrapper methods assess feature subsets by training and evaluating models with different combinations of features.\n",
    "Examples include forward selection (adding features one by one), backward elimination (removing features one by one), and recursive feature elimination (as mentioned earlier).\n",
    "\n",
    "8. Embedded Methods:\n",
    "\n",
    "Some algorithms, like L1-regularized logistic regression or Elastic Net, inherently perform feature selection as part of the model training process.\n",
    "These methods automatically assign low coefficients to irrelevant features.\n",
    "\n",
    "9. SelectKBest:\n",
    "\n",
    "SelectKBest is a method that selects the top 'k' features based on statistical tests such as chi-squared, F-statistic, or mutual information.\n",
    "\n",
    "How Feature Selection Improves Model Performance:\n",
    "\n",
    "1. Reduces Overfitting: Removing irrelevant or redundant features reduces the model's complexity and decreases the risk of overfitting. This results in a model that generalizes better to unseen data.\n",
    "\n",
    "2. Faster Training: Fewer features mean less computation during training, resulting in shorter training times.\n",
    "\n",
    "3. Enhances Model Interpretability: A model with fewer features is easier to interpret and explain to stakeholders.\n",
    "\n",
    "4. Improved Generalization: By focusing on the most informative features, the model can capture the underlying patterns in the data more effectively, leading to better generalization.\n",
    "\n",
    "5. Stability: Feature selection can make the model more robust by reducing noise from irrelevant features, making it less susceptible to changes in the data.\n",
    "\n",
    "6. Efficient Resource Utilization: In applications where resources are limited (e.g., memory, processing power), feature selection can make the model more deployable and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96d7e2-1485-4bd4-840b-5dd9c811ba7b",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing \n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358a918-84e0-49fe-be55-c6eedb319490",
   "metadata": {},
   "source": [
    "Handling imbalanced datasets in logistic regression is essential because logistic regression models can be biased toward the majority class when one class is significantly more prevalent than the other. This can lead to poor predictive performance, especially for the minority class. Here are some strategies for dealing with class imbalance in logistic regression:\n",
    "\n",
    "1. Resampling Techniques:\n",
    "\n",
    "a. Oversampling the Minority Class:\n",
    "\n",
    "Generate additional samples for the minority class to balance the class distribution.\n",
    "Common oversampling techniques include random oversampling, SMOTE (Synthetic Minority Over-sampling Technique), and ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "b. Undersampling the Majority Class:\n",
    "\n",
    "Reduce the number of samples in the majority class to balance the class distribution.\n",
    "Random undersampling and Tomek links are examples of undersampling techniques.\n",
    "\n",
    "2. Generate Synthetic Samples:\n",
    "\n",
    "Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic data points for the minority class based on the existing data distribution. This helps increase the representation of the minority class without collecting new data.\n",
    "\n",
    "3. Cost-Sensitive Learning:\n",
    "\n",
    "Assign different misclassification costs to the minority and majority classes during model training. This way, the model is penalized more for misclassifying the minority class, making it more sensitive to that class.\n",
    "\n",
    "4. Ensemble Methods:\n",
    "\n",
    "Use ensemble techniques like Random Forest, AdaBoost, or XGBoost, which can handle class imbalance naturally. These algorithms can give more weight to the minority class and create diverse base models to improve prediction accuracy.\n",
    "\n",
    "5. Change Decision Threshold:\n",
    "\n",
    "By default, logistic regression uses a threshold of 0.5 to classify samples into classes. You can adjust this threshold to a value that minimizes misclassification costs. For instance, you can lower the threshold to increase sensitivity (recall) for the minority class.\n",
    "\n",
    "6. Anomaly Detection:\n",
    "\n",
    "Treat the minority class as an anomaly detection problem, where the majority class represents the \"normal\" class. Techniques like One-Class SVM or isolation forests can be useful for this approach.\n",
    "\n",
    "7. Use Different Evaluation Metrics:\n",
    "\n",
    "In imbalanced datasets, accuracy can be a misleading metric. Consider using evaluation metrics such as precision, recall, F1-score, area under the ROC curve (AUC-ROC), or area under the precision-recall curve (AUC-PR) that are more informative for imbalanced datasets.\n",
    "\n",
    "8. Collect More Data:\n",
    "\n",
    "Whenever possible, collect more data for the minority class to balance the dataset naturally. This may not always be feasible but can be highly effective.\n",
    "\n",
    "9. Combine Oversampling and Undersampling:\n",
    "\n",
    "You can use a combination of oversampling and undersampling techniques to balance the dataset effectively.\n",
    "\n",
    "10. Select Appropriate Algorithms:\n",
    "\n",
    "Consider using algorithms specifically designed for imbalanced datasets, such as cost-sensitive learning methods and resampling techniques in combination with logistic regression.\n",
    "\n",
    "11. Analyze Feature Importance:\n",
    "\n",
    "Analyze feature importance to ensure that irrelevant or redundant features do not negatively impact the model's ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c0ea1-3c01-49ce-945e-8f7655980146",
   "metadata": {},
   "source": [
    "Q7.Can you discuss some common issues and challenges that may arise when implementing logistic \n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity \n",
    "among the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2742c-89fa-4711-89aa-925058c4e69e",
   "metadata": {},
   "source": [
    "\n",
    "Certainly! Implementing logistic regression can come with several common issues and challenges. Here are some of these issues and how they can be addressed:\n",
    "\n",
    "1. Multicollinearity:\n",
    "\n",
    "Issue: Multicollinearity occurs when independent variables in the logistic regression model are highly correlated with each other. This can make it challenging to discern the individual effects of these variables on the target variable.\n",
    "\n",
    "Solution: Address multicollinearity by:\n",
    "Identifying and removing one of the correlated variables.\n",
    "Combining correlated variables into a single composite variable.\n",
    "Using regularization techniques like Ridge (L2) regression, which can help mitigate multicollinearity by shrinking the coefficients of correlated variables.\n",
    "Performing feature selection to choose the most relevant variables and exclude redundant ones.\n",
    "\n",
    "2. Imbalanced Data:\n",
    "\n",
    "Issue: Imbalanced datasets, where one class significantly outnumbers the other, can lead to biased model performance and poor predictive accuracy for the minority class.\n",
    "\n",
    "Solution: Handle imbalanced data using strategies mentioned in a previous response, such as oversampling, undersampling, generating synthetic samples, cost-sensitive learning, or using different evaluation metrics like precision, recall, and F1-score.\n",
    "\n",
    "3. Non-Linear Relationships:\n",
    "\n",
    "Issue: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the target variable. If the relationship is non-linear, logistic regression may not capture it effectively.\n",
    "\n",
    "Solution: Address non-linear relationships by:\n",
    "Transforming independent variables (e.g., using polynomial features).\n",
    "Trying more complex models like decision trees, random forests, or support vector machines if non-linearity is significant.\n",
    "Using generalized linear models (GLMs) with different link functions, such as the logit, probit, or complementary log-log link functions, to accommodate non-linear relationships.\n",
    "\n",
    "4. Outliers:\n",
    "\n",
    "Issue: Outliers can disproportionately influence the logistic regression model's coefficients and predictions.\n",
    "\n",
    "Solution: Handle outliers by:\n",
    "Identifying and investigating outliers using visualization and statistical techniques.\n",
    "Treating or transforming outliers, or considering robust regression techniques that are less sensitive to outliers.\n",
    "\n",
    "5. Missing Data:\n",
    "\n",
    "Issue: Missing data can lead to incomplete information for model training and prediction.\n",
    "Solution: Address missing data by:\n",
    "Imputing missing values using methods like mean imputation, median imputation, or advanced imputation techniques such as k-nearest neighbors (KNN)  imputation.\n",
    "Considering models that can handle missing data directly, like decision trees and random forests.\n",
    "\n",
    "6. Model Interpretability:\n",
    "\n",
    "Issue: Logistic regression provides interpretable coefficients, but complex interactions between variables may be challenging to interpret.\n",
    "\n",
    "Solution: Enhance model interpretability by:\n",
    "Visualizing the coefficients or odds ratios to understand variable importance.\n",
    "Using feature importance techniques (e.g., permutation importance) for variable interpretation.\n",
    "Creating interaction terms or polynomial features to capture complex relationships explicitly.\n",
    "\n",
    "7. Overfitting:\n",
    "\n",
    "Issue: Overfitting occurs when the logistic regression model fits the training data too closely, capturing noise rather than genuine patterns.\n",
    "\n",
    "Solution: Prevent overfitting by:\n",
    "Using regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients.\n",
    "Splitting the dataset into training and validation sets for model evaluation.\n",
    "Tuning hyperparameters carefully, such as the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c522a-f9f0-42d6-adaf-cfb818031070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e566de-0eaa-410c-9281-ccfbf33bfac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7a3bc-03de-4e7b-955c-c6691af94899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c871a-aec3-4f72-9b80-c0629ee1db07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
